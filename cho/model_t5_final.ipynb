{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paanmego@gmail.com Maded by 2024.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5TokenizerFast, AutoConfig\n",
    "from transformers import (T5ForConditionalGeneration, T5TokenizerFast, \n",
    "                          get_cosine_schedule_with_warmup)\n",
    "from typing import List, Dict, Any\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import wandb\n",
    "import torch.distributed as dist\n",
    "import torch.cuda.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../data/\",\n",
    "        \"model_name\": \"lcw99/t5-large-korean-text-summary\",\n",
    "        \"output_dir\": \"./results\"\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1024,\n",
    "        \"decoder_max_len\": 512,\n",
    "        \"special_tokens\": ['#Person1#', '#Person#', '#Person4#', '#CarNumber#', '#Person2#', '#SSN#', '#Person6#',\n",
    "                            '#DateOfBirth#', '#Email#', '#PhoneNumber#', '#Address#', '#Person3#', '#CardNumber#',\n",
    "                            '#PassportNumber#', '#Person5#', '#Person7#'],\n",
    "        'preserve_special_tokens': True,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"per_device_train_batch_size\": 2,\n",
    "        \"per_device_eval_batch_size\": 2,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"logging_steps\": 3000,\n",
    "        \"eval_steps\": 3000,\n",
    "        \"save_steps\": 'epoch',\n",
    "        \"save_total_limit\": 5,\n",
    "        \"fp16\": True,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"generation_max_length\": 256,\n",
    "        #\"early_stopping_patience\": 3,\n",
    "        #\"early_stopping_threshold\": 0.001,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"result_path\": \"./prediction/\",\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"early_stopping\": True,\n",
    "        \"generate_max_length\": 512, #256\n",
    "        \"num_beams\": 11, #4\n",
    "        \"batch_size\": 1,\n",
    "        \"remove_tokens\": ['<usr>', '</s>', '<s>', '<pad>'],\n",
    "        \"ckt_path\": \"./results/final_t5_model.pt\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 메시지 무시 및 로깅 설정\n",
    "warnings.filterwarnings(\"ignore\", message=\".*resume_download.*\", category=FutureWarning)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "file_handler = logging.FileHandler('training.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# CUDA 설정 : 성능 최적화\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 11:41:49,590 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaanmego\u001b[0m (\u001b[33mdl12\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/upstage-nlp-summarization-nlp12/cho/wandb/run-20240910_114150-hsqps72v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dl12/lm/runs/hsqps72v' target=\"_blank\">lcw99/t5-large-final</a></strong> to <a href='https://wandb.ai/dl12/lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dl12/lm' target=\"_blank\">https://wandb.ai/dl12/lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dl12/lm/runs/hsqps72v' target=\"_blank\">https://wandb.ai/dl12/lm/runs/hsqps72v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dl12/lm/runs/hsqps72v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff5b609f580>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wandb 초기화 : 자신에 맞게 수정\n",
    "wandb.init(\n",
    "    entity=\"dl12\",\n",
    "    project=\"lm\",\n",
    "    name=\"lcw99/t5-large-final\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오래된 로그 파일 삭제\n",
    "def clear_logs():\n",
    "    log_file = 'training.log'\n",
    "    if os.path.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "    logger.info(\"Training log cleared.\")\n",
    "\n",
    "# 오래된 체크포인트 자동 삭제(n개만 유지)\n",
    "def cleanup_old_checkpoints(config: Dict[str, Any], keep_last_n: int = 3):\n",
    "    checkpoint_dir = os.path.join(config['general']['output_dir'], \"checkpoints\")\n",
    "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_\") and f.endswith(\".pt\")], reverse=True)\n",
    "    for checkpoint in checkpoint_files[keep_last_n:]:\n",
    "        os.remove(os.path.join(checkpoint_dir, checkpoint))\n",
    "        logger.info(f\"Removed old checkpoint: {checkpoint}\")\n",
    "\n",
    "# 시퀀스를 작은 청크로 나누는 함수\n",
    "def chunk_sequence(sequence, chunk_size):\n",
    "    return [sequence[i:i + chunk_size] for i in range(0, len(sequence), chunk_size)]\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, tokenizer: T5TokenizerFast):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos_token = tokenizer.bos_token or ''\n",
    "        self.eos_token = tokenizer.eos_token or ''\n",
    "        self.chunk_size = 512\n",
    "\n",
    "    @staticmethod\n",
    "    def make_set_as_df(file_path: str, is_train: bool = True) -> pd.DataFrame:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        replacements = {\n",
    "            'ㅋㅋ': '웃기다', 'ㅇ로': '으로', '제ㅏ': '제가', 'ㅍ알': ' 알', 'ㄷ거': '거',\n",
    "            '##': '#', '회사 #에서': '회사에서',\n",
    "            '#작은': '#Person2#: 작은', '#여기서': '#Person1#: 여기서', '#나': '#Person2#: 나',\n",
    "            '#페리에와': '#Person1#: 페리에와', '#샐러드용': '#Person1#: 샐러드용',\n",
    "            '#어디': '#Person1#: 어디', '#잠깐만요': '#Person1#: 잠깐만요',\n",
    "            '#하지만': '#Person1#: 하지만', '#사람1만기': '#Person1#: 만기',\n",
    "            '#PhoneNumber이고': '#PhoneNumber#이고', '#Person1:': '#Person1#:',\n",
    "            '#Person2:': '#Person2#:', '#Person#': '#Person2#:', '사람1#:': '#Person1#:',\n",
    "            '#고객님:': '#Person2#: 고객님', '선생님: ': '', '로저스 씨: ': '',\n",
    "            '남자: 아악.': '', '남자: 고마워.': ''\n",
    "        }\n",
    "        \n",
    "        df['dialogue'] = df['dialogue'].replace(replacements, regex=True)\n",
    "\n",
    "        if 'summary' in df.columns:\n",
    "            summary_replacements = {\n",
    "                '사람1#': '#Person1#', '사람2#': '#Person2#', '#사람1#': '#Person1#'\n",
    "            }\n",
    "            df['summary'] = df['summary'].replace(summary_replacements, regex=True)\n",
    "\n",
    "        if is_train:\n",
    "            return df[['fname', 'dialogue', 'summary', 'topic']]\n",
    "        else:\n",
    "            return df[['fname', 'dialogue']]\n",
    "\n",
    "    # 모델 입력 생성 : 대화를 청크로 나눔, 각 청크에 대해 프롬프트 생성\n",
    "    def make_input(self, dataset: pd.DataFrame, is_test: bool = False):\n",
    "        encoder_input, decoder_input, decoder_output = [], [], []\n",
    "\n",
    "        for _, row in dataset.iterrows():\n",
    "            dialogue = str(row['dialogue'])\n",
    "            summary = str(row['summary']) if not is_test else \"\"\n",
    "            \n",
    "            for chunk in [dialogue[i:i+self.chunk_size] for i in range(0, len(dialogue), self.chunk_size)]:\n",
    "                encoder_input.append(create_prompt(chunk))\n",
    "                if not is_test:\n",
    "                    decoder_input.append(self.bos_token + summary)\n",
    "                    decoder_output.append(summary + self.eos_token)\n",
    "                else:\n",
    "                    decoder_input.append(self.bos_token)\n",
    "                    decoder_output.append(\"\")\n",
    "\n",
    "        return (encoder_input, decoder_input, decoder_output) if not is_test else (encoder_input, decoder_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS 모델을 상속받아 긴 시퀀스를 처리하도록 확장\n",
    "class LongformerEncoderDecoderForConditionalGeneration(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "# 배치 내의 시퀀스 길이를 동적으로 패딩\n",
    "class DynamicPaddingCollator:\n",
    "    def __init__(self, pad_token_id: int, label_pad_token_id: int):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        if all(len(x[\"input_ids\"]) == len(features[0][\"input_ids\"]) for x in features):\n",
    "            return self.stack_tensors(features)\n",
    "        else:\n",
    "            return self.pad_tensors(features)\n",
    "    \n",
    "    # 모든 텐서를 스택\n",
    "    def stack_tensors(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    # 텐서 크기가 동일한 경우만 스택\n",
    "        try:\n",
    "            return {k: torch.stack([torch.tensor(f[k]) if isinstance(f[k], list) else f[k] for f in features]) for k in features[0].keys()}\n",
    "        except RuntimeError as e:\n",
    "            # 크기가 다른 텐서가 있는 경우 예외 처리\n",
    "            if \"stack expects each tensor to be equal size\" in str(e):\n",
    "                return self.pad_tensors(features)\n",
    "            else:\n",
    "                raise e    \n",
    "            \n",
    "    # 가장 긴 시퀀스에 맞춰 나머지 시퀀스를 패딩\n",
    "    def pad_tensors(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        padded_features = {}\n",
    "        for key in features[0].keys():\n",
    "            padding_value = self.label_pad_token_id if key == \"labels\" else self.pad_token_id\n",
    "            # 리스트를 텐서로 변환하여 패딩 처리\n",
    "            padded_features[key] = torch.nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(f[key]) if isinstance(f[key], list) else f[key] for f in features], \n",
    "                batch_first=True, \n",
    "                padding_value=padding_value\n",
    "            )\n",
    "        return padded_features\n",
    "\n",
    "def create_optimized_dataloaders(config, tokenizer, preprocess):\n",
    "    train_df = preprocess.make_set_as_df(os.path.join(config['general']['data_path'], 'train.csv'), is_train=True)\n",
    "    val_df = preprocess.make_set_as_df(os.path.join(config['general']['data_path'], 'dev.csv'), is_train=True)\n",
    "\n",
    "    train_encoder_input, train_decoder_input, train_decoder_output = preprocess.make_input(train_df, is_test=False)\n",
    "    val_encoder_input, val_decoder_input, val_decoder_output = preprocess.make_input(val_df, is_test=False)\n",
    "\n",
    "    train_dataset = HFDataset.from_dict({\n",
    "        \"encoder_input\": train_encoder_input,\n",
    "        \"decoder_input\": train_decoder_input,\n",
    "        \"decoder_output\": train_decoder_output\n",
    "    })\n",
    "\n",
    "    val_dataset = HFDataset.from_dict({\n",
    "        \"encoder_input\": val_encoder_input,\n",
    "        \"decoder_input\": val_decoder_input,\n",
    "        \"decoder_output\": val_decoder_output\n",
    "    })\n",
    "\n",
    "    # 입력과 레이블을 토큰화\n",
    "    def preprocess_function(examples):\n",
    "        model_inputs = tokenizer(examples[\"encoder_input\"], max_length=config['tokenizer']['encoder_max_len'], truncation=True)\n",
    "        labels = tokenizer(text_target=examples[\"decoder_output\"], max_length=config['tokenizer']['decoder_max_len'], truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=train_dataset.column_names)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=val_dataset.column_names)\n",
    "\n",
    "    # 분산 학습을 위한 샘플러 설정\n",
    "    train_sampler = DistributedSampler(train_dataset) if dist.is_initialized() else None\n",
    "    val_sampler = DistributedSampler(val_dataset, shuffle=False) if dist.is_initialized() else None\n",
    "\n",
    "    collator = DynamicPaddingCollator(pad_token_id=tokenizer.pad_token_id, label_pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['training']['per_device_train_batch_size'], \n",
    "                                  sampler=train_sampler, collate_fn=collator, pin_memory=True, num_workers=4)\n",
    "    eval_dataloader = DataLoader(val_dataset, batch_size=config['training']['per_device_eval_batch_size'], \n",
    "                                 sampler=val_sampler, collate_fn=collator, pin_memory=True, num_workers=4)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, step, loss, config, is_best=False):\n",
    "    checkpoint_dir = os.path.join(config['general']['output_dir'], \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}_step_{step}.pt\")\n",
    "    \n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_state_dict = model.module.state_dict()\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'loss': loss\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    if is_best:\n",
    "        best_model_path = os.path.join(config['general']['output_dir'], \"best_t5_model.pt\")\n",
    "        shutil.copyfile(checkpoint_path, best_model_path)\n",
    "    \n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def load_tokenizer(config):\n",
    "    tokenizer_path = os.path.join(config['general']['output_dir'], \"final_t5_tokenizer\")\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(tokenizer_path)\n",
    "    logger.info(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "    return tokenizer\n",
    "\n",
    "def save_tokenizer(tokenizer, output_dir):\n",
    "    tokenizer_save_path = os.path.join(output_dir, \"final_t5_tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_save_path)\n",
    "    logger.info(f\"Final tokenizer saved at {tokenizer_save_path}\")\n",
    "\n",
    "def load_tokenizer_and_model(config, device, for_inference=False):\n",
    "    logger.info(f\"{'Loading tokenizer & model for inference' if for_inference else 'Loading tokenizer & model for training'}\")\n",
    "    model_name = config['general']['model_name']\n",
    "    \n",
    "    if for_inference:\n",
    "        tokenizer = load_tokenizer(config)\n",
    "        model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(\n",
    "            os.path.join(config['general']['output_dir'], \"final_t5_model\")\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "        special_tokens = config['tokenizer'].get('special_tokens', [])\n",
    "        if special_tokens:\n",
    "            special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            logger.info(f\"Added {num_added_toks} special tokens: {special_tokens}\")\n",
    "\n",
    "        model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        if num_added_toks > 0:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            logger.info(f\"Model embeddings resized to accommodate {num_added_toks} new tokens\")\n",
    "    \n",
    "    tokenizer.do_not_trim_special_tokens = True\n",
    "    \n",
    "    model.to(device)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model(config, device, for_inference=False, checkpoint_path=None):\n",
    "    logger.info(f\"{'Loading tokenizer & model for inference' if for_inference else 'Loading tokenizer & model for training'}\")\n",
    "    model_name = config['general']['model_name']\n",
    "    \n",
    "    if for_inference:\n",
    "        tokenizer = load_tokenizer(config)\n",
    "        \n",
    "        if checkpoint_path:\n",
    "            logger.info(f\"Loading model from checkpoint: {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            \n",
    "            # 체크포인트에서 config를 로드\n",
    "            config = checkpoint.get('config', config)\n",
    "            \n",
    "            # 체크포인트의 vocab_size를 사용하여 모델 초기화\n",
    "            model_config = AutoConfig.from_pretrained(model_name)\n",
    "            model_config.vocab_size = checkpoint['model_state_dict']['shared.weight'].shape[0]\n",
    "            \n",
    "            model = LongformerEncoderDecoderForConditionalGeneration(model_config)\n",
    "            \n",
    "            # 모델의 state_dict를 직접 로드\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "            # tokenizer의 vocab도 업데이트\n",
    "            tokenizer.resize_token_embeddings(model_config.vocab_size)\n",
    "        else:\n",
    "            logger.info(f\"Loading model from final T5 model directory\")\n",
    "            model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(\n",
    "                os.path.join(config['general']['output_dir'], \"final_t5_model\")\n",
    "            )\n",
    "    else:\n",
    "        tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "        special_tokens = config['tokenizer'].get('special_tokens', [])\n",
    "        if special_tokens:\n",
    "            special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            logger.info(f\"Added {num_added_toks} special tokens: {special_tokens}\")\n",
    "\n",
    "        model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        if 'num_added_toks' in locals() and num_added_toks > 0:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "            logger.info(f\"Model embeddings resized to accommodate {num_added_toks} new tokens\")\n",
    "    \n",
    "    tokenizer.do_not_trim_special_tokens = True\n",
    "    \n",
    "    model.to(device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 평가 함수\n",
    "def evaluate(model: torch.nn.Module, dataloader: DataLoader, tokenizer: T5TokenizerFast, device: torch.device, config: Dict[str, Any]):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", dynamic_ncols=True, ascii=True):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=config['training']['generation_max_length'],\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.0,\n",
    "                min_length=10,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            all_preds.extend([pred.strip() for pred in decoded_preds])\n",
    "            all_labels.extend([label.strip() for label in decoded_labels])\n",
    "            \n",
    "            del input_ids, attention_mask, labels, outputs, generated_ids\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "     # 평균 손실 계산\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "     # ROUGE 점수 계산\n",
    "    rouge = Rouge()\n",
    "    if all_preds and all_labels:\n",
    "        try:\n",
    "            scores = rouge.get_scores(all_preds, all_labels, avg=True)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error in ROUGE calculation: {str(e)}\")\n",
    "            logger.info(f\"Sample predictions: {all_preds[:5]}\")\n",
    "            logger.info(f\"Sample labels: {all_labels[:5]}\")\n",
    "            scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    else:\n",
    "        logger.warning(\"No valid predictions or labels for ROUGE calculation.\")\n",
    "        scores = {'rouge-1': {'f': 0.0}, 'rouge-2': {'f': 0.0}, 'rouge-l': {'f': 0.0}}\n",
    "    \n",
    "    # 결과 로깅\n",
    "    logger.info(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    logger.info(\"ROUGE Scores:\")\n",
    "    for k, v in scores.items():\n",
    "        logger.info(f\"  {k}: {v['f']:.4f}\")\n",
    "    \n",
    "    return avg_loss, scores\n",
    "\n",
    "# 특수 토큰 제거\n",
    "def remove_special_tokens(text, remove_tokens):\n",
    "    for token in remove_tokens:\n",
    "        text = text.replace(token, '')\n",
    "    return text\n",
    "\n",
    "def clean_summary(text):\n",
    "    text = re.sub(r'#Person\\d*#', lambda m: f'PERSONTAGPLACEHOLDER{m.group()}PERSONTAGPLACEHOLDER', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'PERSONTAGPLACEHOLDER(#Person\\d*#)PERSONTAGPLACEHOLDER', r'\\1', text)\n",
    "    text = re.sub(r'(#Person\\d*#)(\\s*\\1)+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'(#Person\\d*#)\\s', r'\\1', text)\n",
    "    return text.strip()\n",
    "\n",
    "# 모델 입력용 프롬프트 생성\n",
    "def create_prompt(dialogue):\n",
    "    # Basic\n",
    "    prompt = f\"summarize: {dialogue}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습의 전체 과정 관리 함수\n",
    "def train_and_save(config):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        local_rank = dist.get_rank()\n",
    "        device = torch.device(f'cuda:{local_rank}')\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model, tokenizer = load_tokenizer_and_model(config, device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    \n",
    "    preprocess = Preprocess(tokenizer)\n",
    "    train_dataloader, eval_dataloader = create_optimized_dataloaders(config, tokenizer, preprocess)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['training']['learning_rate'], weight_decay=config['training']['weight_decay'])\n",
    "    num_training_steps = len(train_dataloader) * config['training']['num_train_epochs']\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=config['training']['warmup_steps'], num_training_steps=num_training_steps)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    #early_stopping_callback = EarlyStoppingCallback(patience=config['training']['early_stopping_patience'], threshold=config['training']['early_stopping_threshold'])\n",
    "\n",
    "    best_eval_loss = float('inf')\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(config['training']['num_train_epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", dynamic_ncols=True, ascii=True)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % config['training']['gradient_accumulation_steps'] == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_grad_norm'])\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (not dist.is_initialized() or dist.get_rank() == 0) and step % config['training']['logging_steps'] == 0:\n",
    "                logger.info(f\"Epoch {epoch+1}, Step {step}: Loss {loss.item():.4f}\")\n",
    "                wandb.log({\"train_loss\": loss.item(), \"epoch\": epoch + 1, \"step\": step})\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        if (not dist.is_initialized() or dist.get_rank() == 0) and config['training']['save_steps'] == 'epoch':\n",
    "                save_checkpoint(model, optimizer, epoch, step, loss.item(), config)\n",
    "                cleanup_old_checkpoints(config, keep_last_n=config['training']['save_total_limit'])\n",
    "        \n",
    "        if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "            logger.info(f\"Epoch {epoch+1} - Average train loss: {avg_train_loss:.4f}\")\n",
    "            wandb.log({\"avg_train_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
    "        \n",
    "        eval_loss, rouge_scores = evaluate(model, eval_dataloader, tokenizer, device, config)\n",
    "        if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "            logger.info(f\"Epoch {epoch+1} - Eval Loss: {eval_loss:.4f}\")\n",
    "            logger.info(f\"Epoch {epoch+1} - Rouge Scores:\")\n",
    "            for k, v in rouge_scores.items():\n",
    "                logger.info(f\"  {k.upper()}: {v['f']:.4f}\")\n",
    "        \n",
    "            wandb.log({\n",
    "                \"eval_loss\": eval_loss,\n",
    "                \"rouge-1_f\": rouge_scores['rouge-1']['f'],\n",
    "                \"rouge-2_f\": rouge_scores['rouge-2']['f'],\n",
    "                \"rouge-l_f\": rouge_scores['rouge-l']['f'],\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "        if eval_loss < best_eval_loss:\n",
    "            best_eval_loss = eval_loss\n",
    "            best_model_path = save_checkpoint(model, optimizer, epoch, -1, eval_loss, config, is_best=True)\n",
    "            logger.info(f\"New best model saved with eval loss: {eval_loss:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"No improvement in eval loss. Current best: {best_eval_loss:.4f}\")\n",
    "\n",
    "        # if early_stopping_callback(eval_loss, epoch):\n",
    "        #     logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "        #     if best_model_path:\n",
    "        #         logger.info(f\"Loading best model from {best_model_path}\")\n",
    "        #         checkpoint = torch.load(best_model_path)\n",
    "        #         if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        #             model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #         else:\n",
    "        #             model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #     break\n",
    "    \n",
    "    # 최종 모델 저장 (항상 best model 저장)\n",
    "    final_model_path = os.path.join(config['general']['output_dir'], \"final_t5_model\")\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model.module.save_pretrained(final_model_path)\n",
    "    else:\n",
    "        model.save_pretrained(final_model_path)\n",
    "    logger.info(f\"Final model (best performing) saved at {final_model_path}\")\n",
    "    \n",
    "    # 학습이 끝난 후 토크나이저 저장\n",
    "    save_tokenizer(tokenizer, config['general']['output_dir'])\n",
    "\n",
    "    wandb.finish()\n",
    "    # 로그 파일 지우기\n",
    "    clear_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 11:42:35,627 - INFO - Starting training...\n",
      "2024-09-10 11:42:35,660 - INFO - Loading tokenizer & model for training\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-09-10 11:42:36,132 - INFO - Added 16 special tokens: ['#Person1#', '#Person#', '#Person4#', '#CarNumber#', '#Person2#', '#SSN#', '#Person6#', '#DateOfBirth#', '#Email#', '#PhoneNumber#', '#Address#', '#Person3#', '#CardNumber#', '#PassportNumber#', '#Person5#', '#Person7#']\n",
      "2024-09-10 11:42:37,951 - INFO - Model embeddings resized to accommodate 16 new tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223de919af2a4ed0b35a67961cd2420c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/16306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0f3eeca8b640d1b3eac913acebe91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/8153 [00:00<?, ?it/s]2024-09-10 11:42:44,416 - INFO - Epoch 1, Step 0: Loss 1.9769\n",
      "Epoch 1:  37%|###6      | 3000/8153 [11:36<19:15,  4.46it/s]2024-09-10 11:54:19,667 - INFO - Epoch 1, Step 3000: Loss 1.1723\n",
      "Epoch 1:  74%|#######3  | 6000/8153 [23:11<08:13,  4.36it/s]2024-09-10 12:05:55,283 - INFO - Epoch 1, Step 6000: Loss 0.6646\n",
      "Epoch 1: 100%|##########| 8153/8153 [31:33<00:00,  4.31it/s]\n",
      "2024-09-10 12:14:31,260 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_0_step_8152.pt\n",
      "2024-09-10 12:14:31,261 - INFO - Epoch 1 - Average train loss: 1.5444\n",
      "Evaluating: 100%|##########| 324/324 [07:45<00:00,  1.44s/it]\n",
      "2024-09-10 12:22:17,124 - INFO - Evaluation Loss: 1.0756\n",
      "2024-09-10 12:22:17,124 - INFO - ROUGE Scores:\n",
      "2024-09-10 12:22:17,125 - INFO -   rouge-1: 0.2510\n",
      "2024-09-10 12:22:17,126 - INFO -   rouge-2: 0.0860\n",
      "2024-09-10 12:22:17,126 - INFO -   rouge-l: 0.2406\n",
      "2024-09-10 12:22:17,127 - INFO - Epoch 1 - Eval Loss: 1.0756\n",
      "2024-09-10 12:22:17,128 - INFO - Epoch 1 - Rouge Scores:\n",
      "2024-09-10 12:22:17,128 - INFO -   ROUGE-1: 0.2510\n",
      "2024-09-10 12:22:17,129 - INFO -   ROUGE-2: 0.0860\n",
      "2024-09-10 12:22:17,129 - INFO -   ROUGE-L: 0.2406\n",
      "2024-09-10 12:22:37,790 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_0_step_-1.pt\n",
      "2024-09-10 12:22:37,791 - INFO - New best model saved with eval loss: 1.0756\n",
      "Epoch 2:   0%|          | 0/8153 [00:00<?, ?it/s]2024-09-10 12:22:38,157 - INFO - Epoch 2, Step 0: Loss 1.2677\n",
      "Epoch 2:  37%|###6      | 3000/8153 [11:36<19:53,  4.32it/s]2024-09-10 12:34:14,520 - INFO - Epoch 2, Step 3000: Loss 0.9632\n",
      "Epoch 2:  74%|#######3  | 6000/8153 [23:18<08:00,  4.48it/s]2024-09-10 12:45:56,429 - INFO - Epoch 2, Step 6000: Loss 0.6105\n",
      "Epoch 2: 100%|##########| 8153/8153 [31:41<00:00,  4.29it/s]\n",
      "2024-09-10 12:54:35,920 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_1_step_8152.pt\n",
      "2024-09-10 12:54:35,922 - INFO - Epoch 2 - Average train loss: 1.0703\n",
      "Evaluating: 100%|##########| 324/324 [07:52<00:00,  1.46s/it]\n",
      "2024-09-10 13:02:28,771 - INFO - Evaluation Loss: 1.0203\n",
      "2024-09-10 13:02:28,771 - INFO - ROUGE Scores:\n",
      "2024-09-10 13:02:28,772 - INFO -   rouge-1: 0.2532\n",
      "2024-09-10 13:02:28,773 - INFO -   rouge-2: 0.0898\n",
      "2024-09-10 13:02:28,773 - INFO -   rouge-l: 0.2439\n",
      "2024-09-10 13:02:28,774 - INFO - Epoch 2 - Eval Loss: 1.0203\n",
      "2024-09-10 13:02:28,775 - INFO - Epoch 2 - Rouge Scores:\n",
      "2024-09-10 13:02:28,775 - INFO -   ROUGE-1: 0.2532\n",
      "2024-09-10 13:02:28,776 - INFO -   ROUGE-2: 0.0898\n",
      "2024-09-10 13:02:28,777 - INFO -   ROUGE-L: 0.2439\n",
      "2024-09-10 13:02:55,207 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_1_step_-1.pt\n",
      "2024-09-10 13:02:55,209 - INFO - New best model saved with eval loss: 1.0203\n",
      "Epoch 3:   0%|          | 0/8153 [00:00<?, ?it/s]2024-09-10 13:02:55,574 - INFO - Epoch 3, Step 0: Loss 1.1462\n",
      "Epoch 3:  37%|###6      | 3000/8153 [11:37<20:07,  4.27it/s]2024-09-10 13:14:33,139 - INFO - Epoch 3, Step 3000: Loss 0.8826\n",
      "Epoch 3:  74%|#######3  | 6000/8153 [23:17<08:02,  4.46it/s]2024-09-10 13:26:12,935 - INFO - Epoch 3, Step 6000: Loss 0.5260\n",
      "Epoch 3: 100%|##########| 8153/8153 [31:37<00:00,  4.30it/s]\n",
      "2024-09-10 13:34:44,712 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_2_step_8152.pt\n",
      "2024-09-10 13:34:44,714 - INFO - Epoch 3 - Average train loss: 0.9862\n",
      "Evaluating: 100%|##########| 324/324 [07:58<00:00,  1.48s/it]\n",
      "2024-09-10 13:42:43,419 - INFO - Evaluation Loss: 1.0008\n",
      "2024-09-10 13:42:43,420 - INFO - ROUGE Scores:\n",
      "2024-09-10 13:42:43,421 - INFO -   rouge-1: 0.2590\n",
      "2024-09-10 13:42:43,421 - INFO -   rouge-2: 0.0950\n",
      "2024-09-10 13:42:43,422 - INFO -   rouge-l: 0.2488\n",
      "2024-09-10 13:42:43,423 - INFO - Epoch 3 - Eval Loss: 1.0008\n",
      "2024-09-10 13:42:43,423 - INFO - Epoch 3 - Rouge Scores:\n",
      "2024-09-10 13:42:43,424 - INFO -   ROUGE-1: 0.2590\n",
      "2024-09-10 13:42:43,424 - INFO -   ROUGE-2: 0.0950\n",
      "2024-09-10 13:42:43,425 - INFO -   ROUGE-L: 0.2488\n",
      "2024-09-10 13:43:11,109 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_2_step_-1.pt\n",
      "2024-09-10 13:43:11,111 - INFO - New best model saved with eval loss: 1.0008\n",
      "Epoch 4:   0%|          | 0/8153 [00:00<?, ?it/s]2024-09-10 13:43:11,536 - INFO - Epoch 4, Step 0: Loss 1.0244\n",
      "Epoch 4:  37%|###6      | 3000/8153 [11:38<19:25,  4.42it/s]2024-09-10 13:54:49,597 - INFO - Epoch 4, Step 3000: Loss 0.8699\n",
      "Epoch 4:  74%|#######3  | 6000/8153 [23:13<08:10,  4.39it/s]2024-09-10 14:06:24,852 - INFO - Epoch 4, Step 6000: Loss 0.4793\n",
      "Epoch 4: 100%|##########| 8153/8153 [31:33<00:00,  4.31it/s]\n",
      "2024-09-10 14:14:54,153 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_3_step_8152.pt\n",
      "2024-09-10 14:14:54,154 - INFO - Epoch 4 - Average train loss: 0.9284\n",
      "Evaluating: 100%|##########| 324/324 [08:11<00:00,  1.52s/it]\n",
      "2024-09-10 14:23:06,072 - INFO - Evaluation Loss: 0.9923\n",
      "2024-09-10 14:23:06,073 - INFO - ROUGE Scores:\n",
      "2024-09-10 14:23:06,073 - INFO -   rouge-1: 0.2625\n",
      "2024-09-10 14:23:06,074 - INFO -   rouge-2: 0.0977\n",
      "2024-09-10 14:23:06,074 - INFO -   rouge-l: 0.2530\n",
      "2024-09-10 14:23:06,075 - INFO - Epoch 4 - Eval Loss: 0.9923\n",
      "2024-09-10 14:23:06,076 - INFO - Epoch 4 - Rouge Scores:\n",
      "2024-09-10 14:23:06,076 - INFO -   ROUGE-1: 0.2625\n",
      "2024-09-10 14:23:06,076 - INFO -   ROUGE-2: 0.0977\n",
      "2024-09-10 14:23:06,077 - INFO -   ROUGE-L: 0.2530\n",
      "2024-09-10 14:23:29,863 - INFO - Checkpoint saved at ./results/checkpoints/checkpoint_epoch_3_step_-1.pt\n",
      "2024-09-10 14:23:29,865 - INFO - New best model saved with eval loss: 0.9923\n",
      "Epoch 5:   0%|          | 0/8153 [00:00<?, ?it/s]2024-09-10 14:23:30,216 - INFO - Epoch 5, Step 0: Loss 0.9871\n",
      "Epoch 5:  37%|###6      | 3000/8153 [11:36<19:34,  4.39it/s]2024-09-10 14:35:06,590 - INFO - Epoch 5, Step 3000: Loss 0.8080\n",
      "Epoch 5:  74%|#######3  | 6000/8153 [23:14<08:21,  4.29it/s]2024-09-10 14:46:44,177 - INFO - Epoch 5, Step 6000: Loss 0.3589\n",
      "Epoch 5:  79%|#######8  | 6433/8153 [24:53<06:37,  4.32it/s]"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        torch.cuda.set_device(dist.get_rank())\n",
    "    \n",
    "logger.info(\"Starting training...\")\n",
    "max_retries = 3\n",
    "retry_count = 0\n",
    "\n",
    "# CUDA Out of Memory Exception 처리 (계속 에러가 나는 경우 kill -9 로 백그라운 파이썬 강제 처리 해야함)\n",
    "while retry_count < max_retries:\n",
    "    try:\n",
    "        train_and_save(config)\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            retry_count += 1\n",
    "            logger.warning(f\"CUDA out of memory error occurred. Attempt {retry_count} of {max_retries}.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            if retry_count == max_retries:\n",
    "                logger.error(\"Max retries reached. Exiting.\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.error(\"Unexpected error occurred.\", exc_info=True)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 적용 함수\n",
    "def inference(config, model, tokenizer, preprocessor):\n",
    "    test_data = preprocessor.make_set_as_df(os.path.join(config['general']['data_path'], 'test.csv'), is_train=False)\n",
    "    fnames = test_data['fname'].tolist()\n",
    "    test_data.set_index('fname', inplace=True)\n",
    "    \n",
    "    encoder_input_test = test_data['dialogue'].apply(lambda x: create_prompt(x)).tolist()\n",
    "    \n",
    "    inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                       add_special_tokens=True, truncation=True,\n",
    "                       max_length=config['tokenizer']['encoder_max_len'])\n",
    "    \n",
    "    dataset = list(zip(inputs['input_ids'], inputs['attention_mask'], fnames))\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        input_ids, attention_masks, fnames = zip(*batch)\n",
    "        return torch.stack(input_ids), torch.stack(attention_masks), fnames\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=config['inference']['batch_size'], \n",
    "                            num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, ids in tqdm(dataloader, desc=\"Inference\", dynamic_ncols=True, ascii=True):\n",
    "            text_ids.extend(ids)\n",
    "            \n",
    "            input_ids = input_ids.to(model.device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(model.device, non_blocking=True)\n",
    "            \n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                early_stopping=config['inference']['early_stopping'],\n",
    "                max_length=config['inference']['generate_max_length'],\n",
    "                #min_length=config['inference']['min_length'],\n",
    "                num_beams=config['inference']['num_beams'],\n",
    "                # length_penalty=1.0,\n",
    "                # repetition_penalty=1.1,\n",
    "                do_sample=False,\n",
    "                #temperature=0.3,\n",
    "                #top_k=50,\n",
    "                #top_p=0.9,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "            \n",
    "            decoded_summaries = tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens=False)\n",
    "            processed_summaries = [clean_summary(summary) for summary in decoded_summaries]\n",
    "            summary.extend(processed_summaries)\n",
    "    \n",
    "    output = pd.DataFrame({\"fname\": text_ids, \"summary\": summary})\n",
    "\n",
    "    output_file = os.path.join(config['inference']['result_path'], \"inference_output.csv\")\n",
    "    os.makedirs(config['inference']['result_path'], exist_ok=True)\n",
    "    output.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Inference results saved to: {output_file}\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-10 11:10:26,366 - INFO - Starting inference...\n",
      "2024-09-10 11:10:26,368 - INFO - Loading tokenizer & model for inference\n",
      "2024-09-10 11:10:26,427 - INFO - Tokenizer loaded from ./results/final_t5_tokenizer\n",
      "Inference: 100%|##########| 499/499 [12:23<00:00,  1.49s/it]\n",
      "2024-09-10 11:22:52,230 - INFO - Inference results saved to: ./prediction/inference_output.csv\n",
      "2024-09-10 11:22:52,249 - INFO - Inference completed. Output saved to: ./prediction/inference_output.csv\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting inference...\")\n",
    "try:\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model, tokenizer = load_tokenizer_and_model(config, device, for_inference=True)\n",
    "    #model, tokenizer = load_tokenizer_and_model(config, device, for_inference=True, checkpoint_path=\"./results/checkpoints/checkpoint_epoch_4_step_8152.pt\")\n",
    "    preprocessor = Preprocess(tokenizer)\n",
    "    output = inference(config, model, tokenizer, preprocessor)\n",
    "    logger.info(f\"Inference completed. Output saved to: {os.path.join(config['inference']['result_path'], 'inference_output.csv')}\")\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Model file not found: {str(e)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred during inference: {str(e)}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
