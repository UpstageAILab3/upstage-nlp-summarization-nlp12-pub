# T5: 텍스트-텍스트 변환 모델

Google Research에서 개발한 T5(Text-to-Text Transfer Transformer) 모델

### 텍스트-텍스트 프레임워크: NLP의 통합적 접근
T5의 가장 큰 특징은 모든 NLP 작업을 '텍스트 입력 → 텍스트 출력'의 형태로 통일

기존 접근법의 한계: 전통적으로 NLP 작업들은 각각 다른 방식으로 접근. 예를 들어, 텍스트 분류는 카테고리 예측 문제로, 기계 번역은 시퀀스-투-시퀀스 문제 접근.

T5: T5는 이 모든 작업을 텍스트 변환 문제로 일반화. 번역, 요약, 감성 분석 등 모든 NLP 작업이 동일한 형식으로 처리

실제 예시:

번역: "translate English to German: How are you? → Wie geht es dir?"
요약: "summarize: [긴 문단] → [요약된 내용]"
감성 분석: "classify sentiment: I love this movie! → positive"

이 접근 방식의 장점 하나의 모델로 다양한 NLP 작업을 수행할 수 있게 되어, 모델의 범용성과 효율성이 크게 향상

### Seq2Seq 아키텍처
T5는 Transformer 기반의 Sequence-to-Sequence(Seq2Seq) 아키텍처를 채택. 이 구조는 복잡한 텍스트 변환 작업에 특히 효과적

인코더(Encoder): 입력 텍스트를 읽고 이를 고차원의 벡터 표현으로 변환. 이 과정에서 텍스트의 의미와 문맥 정보가 포착.
디코더(Decoder): 인코더가 생성한 벡터를 기반으로 출력 텍스트를 생성. 이 과정은 단어를 하나씩 순차적으로 생성하며, 이전에 생성된 단어들을 고려하여 다음 단어를 예측.
주의 메커니즘(Attention Mechanism): 인코더와 디코더 사이의 정보 흐름을 조절. 이를 통해 모델은 입력 텍스트의 어느 부분에 집중해야 할지 학습.

### 사전 학습과 미세 조정: 효율적인 학습 전략
T5는 대규모 텍스트 데이터에서의 사전 학습(Pre-training)과 특정 작업을 위한 미세 조정(Fine-tuning)이라는 두 단계 학습 과정

##### 사전 학습 (Pre-training):

Span Corruption: T5는 "span corruption"이라는 특별한 방식으로 사전 학습.
작동 방식: 원본 텍스트에서 연속된 단어 묶음(span)을 무작위로 마스킹하고, 모델이 이를 복원하도록 학습.
예: "The [MASK] [MASK] is blue" → 모델이 "The sky is blue"로 복원하도록 학습
이 방식은 BERT의 마스킹 기법을 확장, 더 긴 문맥을 이해하는 데 효과적.


##### 미세 조정 (Fine-tuning):

사전 학습된 모델을 특정 NLP 작업(예: 번역, 요약)에 맞게 추가로 훈련.
이 과정에서 모델은 일반적인 언어 이해력을 바탕으로 특정 작업에 특화.

### 전이 학습의 강점: 적은 데이터로도 높은 성능

데이터 효율성: 새로운 NLP 작업을 수행할 때, 처음부터 대량의 데이터로 학습할 필요 없이 사전 학습된 지식을 활용.
범용성: 하나의 사전 학습된 모델을 다양한 NLP 작업에 적용할 수 있어, 모델 개발과 배포의 효율성.
성능 향상: 특히 학습 데이터가 부족한 도메인에서도 우수한 성능을 발휘.

### 다양한 모델 크기: 유연한 선택
T5는 다양한 크기의 모델을 제공: Small, Base, Large, 3B, 11B

Small: 가장 작은 모델, 빠른 처리 속도와 적은 계산 자원을 필요
11B: 가장 큰 모델, 최고의 성능을 제공

### 특수 토큰을 통한 태스크 구분
T5는 입력 문장 앞에 토큰을 추가하여 수행할 작업을 명시.

작동 방식: 예를 들어, 대화요약의 경우 "summarized:(혹은 대화를 요약해주세요)" 라는 토큰을 입력 문장 앞에 추가
##### 장점:
하나의 모델이 여러 NLP 작업을 동시에 처리.
모델이 문맥을 더 잘 이해하고, 적절한 출력을 생성하는 데 도움.
새로운 작업을 추가할 때 모델 구조를 변경할 필요 없이 새로운 특수 토큰만 정의.

### 성능 최적화: 벤치마크에서의 우수성
T5는 다양한 NLP 벤치마크에서 최고 수준의 성능. GLUE, SuperGLUE: 이들은 자연어 이해 능력을 평가하는 대표적인 벤치

### Zero-shot & Few-shot 학습
T5의 일반화 능력은 zero-shot 및 few-shot 학습 적용.

##### Zero-shot 학습: 특정 작업에 대한 학습 데이터 없이도 수행 가능
##### Few-shot 학습: 매우 적은 수의 예시만으로도 새로운 작업 수행 가능