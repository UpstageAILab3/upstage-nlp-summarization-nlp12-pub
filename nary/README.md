### Data Processing

- test.csv와 dev.csv 파일에는 오탈자가 거의 없었으나, train.csv에는 꽤 많은 오탈자가 있어서 고쳤다.
- dialogue 오탈자
```
replacements = {
            'ㅋㅋ': '웃기다', 'ㅇ로': '으로', 
            '제ㅏ': '제가', 'ㅍ알': ' 알', 
            'ㄷ거': '거',
            '##': '#', '회사 #에서': '회사에서',
            '#작은': '#Person2#: 작은', '#여기서': '#Person1#: 여기서', 
            '#나': '#Person2#: 나',
            '#페리에와': '#Person1#: 페리에와', 
            '#샐러드용': '#Person1#: 샐러드용',
            '#어디': '#Person1#: 어디', 
            '#잠깐만요': '#Person1#: 잠깐만요',
            '#하지만': '#Person1#: 하지만', 
            '#사람1만기': '#Person1#: 만기',
            '#PhoneNumber이고': '#PhoneNumber#이고', '#Person1:': '#Person1#:',
            '#Person2:': '#Person2#:', '#Person#': '#Person2#:', '사람1#:': '#Person1#:',
            '#고객님:': '#Person2#: 고객님', 
            '선생님: ': '', '로저스 씨: ': '',
            '남자: 아악.': '', '남자: 고마워.': ''
        }
df['dialogue'] = df['dialogue'].replace(replacements, regex=True)
```
- summary 오탈자
```
if 'summary' in df.columns:
    summary_replacements = {
        '사람1#': '#Person1#', '사람2#': '#Person2#', '#사람1#': '#Person1#'
    }
df['summary'] = df['summary'].replace(summary_replacements, regex=True)
```

## 4. Modeling

### Kobart
KoBART는 한국어에 특화된 BART 모델이다. BART는 Facebook AI에서 개발한 sequence-to-sequence 모델로, 주로 텍스트 생성, 요약, 번역 등에 사용된다. KoBART는 이 BART 모델을 바탕으로 하여, 한국어 데이터를 사용해 사전 학습된 모델이다.

BART의 구조는 인코더와 디코더로 이루어져 있는데, 인코더는 입력된 텍스트를 분석하고, 디코더는 그 텍스트를 바탕으로 새로운 텍스트를 생성한다. KoBART는 이러한 구조를 기반으로 하여, 한국어 텍스트의 요약, 생성 등에서 우수한 성능을 보인다.

이를 바탕으로 digit82/kobart_summarization은 KoBART를 활용해 한국어 텍스트 요약 작업을 수행하는 프로젝트이다. 

#### 장점:
- 한국어 특화: KoBART는 한국어 데이터를 바탕으로 학습되었기 때문에, 한국어 문장을 다루는 다양한 작업에서 높은 성능을 보인다. 특히 한국어의 어순이나 문법적 특성을 잘 이해하여 자연스러운 요약과 생성이 가능하다.

- 범용성: KoBART는 BART의 인코더-디코더 구조를 따르기 때문에, 요약뿐만 아니라 번역, 텍스트 생성 등 다양한 자연어 처리 작업에 적용할 수 있다.

- 사전 학습된 모델 활용: 이미 대규모 데이터로 학습된 모델이기 때문에, 추가 학습(fine-tuning)을 통해 특정 도메인에 쉽게 적용할 수 있다. 예를 들어 뉴스 요약이나 문서 생성 같은 작업에 특화된 모델로 빠르게 변환 가능하다.

- Pretrained 모델 지원: KoBART는 이미 공개된 사전 학습된 모델이기 때문에, 별도의 대규모 학습 없이도 바로 사용할 수 있어 효율적이다.

#### 단점:
- 대규모 학습 데이터 필요: KoBART를 특정 작업에 맞춰 미세 조정(fine-tuning)할 때, 여전히 대규모의 학습 데이터가 필요하다. 데이터가 부족하면 모델의 성능이 떨어질 수 있다.

- 한국어 외의 언어 한정성: KoBART는 한국어에 특화되어 있기 때문에, 다른 언어를 다룰 때는 성능이 크게 떨어질 수 있다. 다국어 작업에는 적합하지 않다.

- 메모리와 연산 자원 소모: BART 모델의 특성상 인코더와 디코더가 모두 사용되기 때문에, 훈련 및 추론 과정에서 많은 메모리와 연산 자원이 필요하다. 특히 긴 문장이나 대규모 데이터를 처리할 때 이 문제가 더 두드러질 수 있다.

- 모델 크기: KoBART는 대규모의 파라미터를 가지고 있기 때문에, 실제로 모델을 배포하거나 실시간 작업에 사용하려면 최적화 작업이 필요할 수 있다.

### kobart로 시도한 것들
- 번역어투임을 감안하여 영어로 번역후 bart 요약 후 다시 번역. -> 번역에서의 오류 + 요약에서의 오류가 가중되어 좋은 결과를 얻지 못함. 
- rouge 점수를 이용하여 모델을 업데이트 시키기위해 강화학습 알고리즘을 이용한 학습 
- kfold
- kfold + 강화학습

#### 위의 모든 것들이 baseline을 넘지 못하였다. 
- private 기준으로 가장 좋은 점수를 받았던 모델은 baseline에서 max_length를 1024/512로 바꾼것이다.

### 아쉬웠던 점
- 개인적으로는 계속 kobart만을 파인튜닝하려고 노력했는데, 결과적으로는 잘되지 않았다. 데이터증강을 시도하다가 하지 않았는데 그게 아쉽다. 

#### 시도해보고싶은 점
- 다른 조원들이 LLAMA나 T5등을 파인튜닝하면서 많은 것을 배우신 것 같았다. 대회는 끝나지만, 남은 온라인 수업과 함께 요즘 유행하는 모델들을 공부하고 직접 다루고 싶다. 그렇지만 kobart 만큼은 정말 많이 알고가서 뿌듯하다.